---
title: "Data Science Capstone - Milestone Report"
author: "Ryan Wissman"
date: "September 3, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Summary

The purpose of this document is to describe and summarize the data found within the SwiftKey text data set. Here, I will outline some brief exploratory data analysis of the blogs, twitter, and news datasets as well as provide some initial analysis. This is the first part of the Coursera Data Science capstone and is the first step in creating my prediction algorithm.   

## Loading the Dataset

First we will load the data, which was already downloaded to our computer, as well as any prerequisite libraries. The three basic files that we will use for this analysis are as follows: 

* en_US.blogs.txt
* en_US.news.txt
* en_US.twitter.txt

```{r}
#Load the libraries
library(stringi)
library(tm)
library(wordcloud)

#Load the files
file_blogs = "F:/My Documents/GitHub/Coursera-DataScienceCapstone/SwiftKey_DataFiles/en_US.blogs.txt"
file_news = "F:/My Documents/GitHub/Coursera-DataScienceCapstone/SwiftKey_DataFiles/en_US.news.txt"
file_twitter = "F:/My Documents/GitHub/Coursera-DataScienceCapstone/SwiftKey_DataFiles/en_US.twitter.txt"

dataset_blogs <- readLines(file_blogs, encoding='UTF-8', warn=FALSE, skipNul=TRUE)
dataset_news <- readLines(file_news, encoding='UTF-8', warn=FALSE, skipNul=TRUE)
dataset_twitter <- readLines(file_twitter, encoding='UTF-8', warn=FALSE, skipNul=TRUE)
```

## Basic Statistics - Exploring the Data

First, let's take a look at some of the basics.

### File Sizes

We will first determine the size of the files, in megabytes, to get a basic idea how large these files are.

```{r}
#Get file sizes in MB, divided until bytes are megabytes
size_blogs <- format(round(file.info(file_blogs)$size / 1024 / 1024, digits=1), nsmall=2)
size_news <- format(round(file.info(file_news)$size / 1024 / 1024, digits=1), nsmall=2)
size_twitter <- format(round(file.info(file_twitter)$size / 1024 / 1024, digits=1), nsmall=2)

#Print sizes in table for easy reading
data_size <- as.table(matrix(c(size_blogs, size_news, size_twitter)))
colnames(data_size) <- c("File Size in MB")
rownames(data_size) <- c("Blogs File", "News File", "Twitter File")
data_size
```

### Line Counts

We will now determine the number of lines for each data file. 

```{r}
#Get the number of lines per file
lines_blogs <- length(dataset_blogs)
lines_news <- length(dataset_news)
lines_twitter <- length(dataset_twitter)

#Print number of lines in table for easy reading
data_lines <- as.table(matrix(c(lines_blogs, lines_news, lines_twitter)))
colnames(data_lines) <- c("Number of lines")
rownames(data_lines) <- c("Blogs File", "News File", "Twitter File")
data_lines
```

### Word Counts

Now we will determine how many usable words are in each file. We will do this by using the stri_stats_latex funtion that is found within the stringi library. This will also show us the number of characters. **We will primarily focus on the "Words" column as this will show us how many words are in each file.** 

```{r}
#Determine stats using stri_stats_latex
stats_blogs <- stri_stats_latex(dataset_blogs)
stats_news <- stri_stats_latex(dataset_news)
stats_twitter <- stri_stats_latex(dataset_twitter)

#Print in table for easier reading
data_stats <- rbind(stats_blogs, stats_news, stats_twitter)
data_stats <- as.data.frame(data_stats)
rownames(data_stats) <- c("Blogs File", "News File", "Twitter File")
data_stats
```

It would also be useful to create a plot to visualize which file contains the most words, since words are what we will primarily be focusing on for this project. 

```{r}
#Order the data for column chart
data_stats <- data_stats[order(data_stats$Words),]
barplot(data_stats$Words, names.arg = c("Blogs File", "News File", "Twitter File"), col="light blue",
        main="Number of Words in Each File", xlab="File Name")
```

## Analyzing the Data

From these statics we can see that the blogs and twitter files have the most words whereas the news file has a much lower word count and line count. The twitter file has the largest number of lines, probably due to the nature of twitter itself as it is designed for microbloging a couple of lines at a time with a 140 character max per tweet.  

Next we will dive a little deeper to determine try and gleen some more useful information from these files. Namely we will try to determine and plot the most common individual words and word pairs (bigrams).

### Create Sample Dataset and Combine Files

First we will need to combine these three files into one clean corpus. We wil use the tm package to assist with the data cleaning and transformation. We will also take a small sample of all three files to ease with data processing and corpus creation. Otherwise, the files created become too large for working memory (16GB). We will take about 3% of the data from each file. 

To process these data we will do the following: 

*Clean emoji encoding issues
*Remove unneeded punctuation marks
*Remove numbers
*Remove extra whitespace
*Remove stopwords
*Convert to lowercase only

```{r}
#Create small samples (3%) of all three datasets, this greatly speeds up processing
sample_blogs <- readLines(file_blogs, (lines_blogs*0.03), encoding='UTF-8', warn=FALSE, skipNul=TRUE)
sample_news <- readLines(file_news, (lines_news*0.03), encoding='UTF-8', warn=FALSE, skipNul=TRUE)
sample_twitter <- readLines(file_twitter, (lines_twitter*0.03), encoding='UTF-8')

#Merge the three data sets
dataset_sample <- c(sample_blogs, sample_news, sample_twitter)

#Create a clean corpus sample of all three files
corpus_vector <- VectorSource(dataset_sample)
clean_corpus <- Corpus(corpus_vector)

#Make sure encoding is correct and clean some issues related to emojis in the dataset
clean_corpus <- tm_map(clean_corpus, function(x) iconv(x, to='UTF-8', sub='byte'))

#Clean punctuation marks
clean_corpus <- tm_map(clean_corpus, removePunctuation)

#Remove numbers
clean_corpus <- tm_map(clean_corpus, removeNumbers)

#Remove extra whitespace
clean_corpus <- tm_map (clean_corpus, stripWhitespace)

#Remove english stopwords
clean_corpus <- tm_map(clean_corpus, removeWords, stopwords("english"))

#Convert all letters to lowercase
clean_corpus <- tm_map(clean_corpus, content_transformer(tolower))

#Convert to a plaintext document
clean_corpus <- tm_map(clean_corpus, PlainTextDocument)
```

### Plot Common Words

Now that we have cleaned the data we can now create a couple of simple plots to find the most common words. Let's use a word cloud to determine which words have the highest frequency in the dataset. We need to use the wordcloud package to acomplish this easily. This word cloud is showing the 75 most frequent words.

```{r}
wordcloud(clean_corpus, max.words = 75, colors=brewer.pal(5, "Set1"))
```

We can see from the word cloud most common words are things like **the**, **like**, **just**, **get**, and **can**.

##N-Grams

The next step before creating our prediction algorithm, and to assist with exploring the data, we will create a function to tokenize n-grams. N-grams ar: *"a contiguous sequence of n items from a given sequence of text or speech."* We can then use the function to find the top unigrams and bigrams (one word and two word pairs).

### Unigrams

First we will use the functionality found within the tm package to create a Term Document Matrix. A Term Document Matrix is used to better understand our data and assist us in analyzing the frequency of words. First we will create a Term Document Matrix and use it for unigrams tokenization. 

```{r}
#Unigram tonkenizer, code adapted from R FAQ on bigrams, see references
UnigramTokenizer <- function(x) unlist(lapply(ngrams(words(x), 1), paste, collapse = " "), use.names = FALSE)
uni_tdm <- TermDocumentMatrix(clean_corpus, control = list(tokenize = UnigramTokenizer))

#Memory will overload if we do not remove terms with low frequency
uni_tdm <- (removeSparseTerms(uni_tdm, 0.9999))

#Get the frequency of words (unigrams)
uni_frequency <- sort(rowSums(as.matrix(uni_tdm)), decreasing=TRUE)
```

Let's create a plot of the frequency of unigrams:

```{r}
barplot(uni_frequency, col="light blue",
        main="Highest Frequency Unigrams", xlab="Word", ylab="Frequency")
```

This matches up nicely with the word cloud that we created, it seems that we are on the right path. Please note that words like **and** are missing as they are sopwords and were removed for this exercise. 

### Bigrams

Now let's determine the frequency of the most common bigrams (word pairs) using the same code as above (for unigrams), with only a slight modification. 

```{r}
#Unigram tonkenizer, code adapted from R FAQ on bigrams, see references
UnigramTokenizer <- function(x) unlist(lapply(ngrams(words(x), 1), paste, collapse = " "), use.names = FALSE)
uni_tdm <- TermDocumentMatrix(clean_corpus, control = list(tokenize = UnigramTokenizer))

#Memory will overload if we do not remove terms with low frequency
uni_tdm <- (removeSparseTerms(uni_tdm, 0.9999))

#Get the frequency of words (unigrams)
uni_frequency <- sort(rowSums(as.matrix(uni_tdm)), decreasing=TRUE)
```

Let's create a plot of the frequency of unigrams:

```{r}
barplot(uni_frequency, col="light blue",
        main="Highest Frequency Unigrams", xlab="Word", ylab="Frequency")
```

This matches up nicely with the word cloud that we created, it seems that we are on the right path. Please note that words like **and** are missing as they are sopwords and were removed for this exercise. 


## References

* This report uses the SwiftKey dataset which can be downloaded here: http://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip 
* Wikipedia page on n-gram: https://en.wikipedia.org/wiki/N-gram 
* Bigram R code: http://tm.r-forge.r-project.org/faq.html#Bigrams 