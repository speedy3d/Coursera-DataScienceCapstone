---
title: "Data Science Capstone - Milestone Report"
author: "Ryan Wissman"
date: "September 3, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Summary

The purpose of this document is to describe and summarize the data found within the SwiftKey text data set. Here, I will outline some brief exploratory data analysis of the blogs, twitter, and news datasets as well as provide some initial analysis. This is the first part of the Coursera Data Science capstone and is the first step in creating my prediction algorithm.   

## Loading the Dataset

First we will load the data, which was already downloaded to our computer, as well as any prerequisite libraries. The three basic files that we will use for this analysis are as follows: 

* en_US.blogs.txt
* en_US.news.txt
* en_US.twitter.txt

```{r}
#Load the libraries
library(stringi)
library(tm)

#Load the files
file_blogs = "F:/My Documents/GitHub/Coursera-DataScienceCapstone/SwiftKey_DataFiles/en_US.blogs.txt"
file_news = "F:/My Documents/GitHub/Coursera-DataScienceCapstone/SwiftKey_DataFiles/en_US.news.txt"
file_twitter = "F:/My Documents/GitHub/Coursera-DataScienceCapstone/SwiftKey_DataFiles/en_US.twitter.txt"

dataset_blogs <- readLines(file_blogs, encoding='UTF-8', warn=FALSE, skipNul=TRUE)
dataset_news <- readLines(file_news, encoding='UTF-8', warn=FALSE, skipNul=TRUE)
dataset_twitter <- readLines(file_twitter, encoding='UTF-8', warn=FALSE, skipNul=TRUE)
```

## Basic Statistics - Exploring the Data

First, let's take a look at some of the basics.

### File Sizes

We will first determine the size of the files, in megabytes, to get a basic idea how large these files are.

```{r}
#Get file sizes in MB, divided until bytes are megabytes
size_blogs <- format(round(file.info(file_blogs)$size / 1024 / 1024, digits=1), nsmall=2)
size_news <- format(round(file.info(file_news)$size / 1024 / 1024, digits=1), nsmall=2)
size_twitter <- format(round(file.info(file_twitter)$size / 1024 / 1024, digits=1), nsmall=2)

#Print sizes in table for easy reading
data_size <- as.table(matrix(c(size_blogs, size_news, size_twitter)))
colnames(data_size) <- c("File Size in MB")
rownames(data_size) <- c("Blogs File", "News File", "Twitter File")
data_size
```

### Line Counts

We will now determine the number of lines for each data file. 

```{r}
#Get the number of lines per file
lines_blogs <- length(dataset_blogs)
lines_news <- length(dataset_news)
lines_twitter <- length(dataset_twitter)

#Print number of lines in table for easy reading
data_lines <- as.table(matrix(c(lines_blogs, lines_news, lines_twitter)))
colnames(data_lines) <- c("Number of lines")
rownames(data_lines) <- c("Blogs File", "News File", "Twitter File")
data_lines
```

### Word Counts

Now we will determine how many usable words are in each file. We will do this by using the stri_stats_latex funtion that is found within the stringi library. This will also show us the number of characters. **We will primarily focus on the "Words" column as this will show us how many words are in each file.** 

```{r}
#Determine stats using stri_stats_latex
stats_blogs <- stri_stats_latex(dataset_blogs)
stats_news <- stri_stats_latex(dataset_news)
stats_twitter <- stri_stats_latex(dataset_twitter)

#Print in table for easier reading
data_stats <- rbind(stats_blogs, stats_news, stats_twitter)
data_stats <- as.data.frame(data_stats)
rownames(data_stats) <- c("Blogs File", "News File", "Twitter File")
data_stats
```

It would also be useful to create a plot to visualize which file contains the most words, since words are what we will primarily be focusing on for this project. 

```{r}
#Order the data for column chart
data_stats <- data_stats[order(data_stats$Words),]
barplot(data_stats$Words, names.arg = c("Blogs File", "News File", "Twitter File"), col="blue",
        main="Number of Words in Each File", xlab="File Name")
```

## Analyzing the Data

From these statics we can see that the blogs and twitter files have the most words whereas the news file has a much lower word count and line count. The twitter file has the largest number of lines, probably due to the nature of twitter itself as it is designed for microbloging a couple of lines at a time with a 140 character max per tweet.  

Next we will dive a little deeper to determine try and gleen some more useful information from these files. Namely we will try to determine and plot the most common individual words and word pairs (bigrams).

### Create Sample and Combine Files

First we will need to combine these three files into one clean corpus. We wil use the tm package to assist with the data cleaning and transformation. We will also take a small sample of all three files to ease with data processing and corpus creation. Otherwise, the files created become too large for working memory (16GB). We will take about 5% of the data from each file. 

To process these data we will do the following: 

*Remove unneeded punctuation marks
*Remove numbers
*Remove extra whitespace
*Convert to lowercase only

```{r}
#Create small samples (5%) of all three datasets
sample_blogs <- readLines(file_blogs, (lines_blogs*0.05), encoding='UTF-8', warn=FALSE, skipNul=TRUE)
sample_news <- readLines(file_news, (lines_news*0.05), encoding='UTF-8', warn=FALSE, skipNul=TRUE)
sample_twitter <- readLines(file_twitter, (lines_twitter*0.05), encoding='UTF-8', warn=FALSE, skipNul=TRUE)

#Merge the three data sets
dataset_sample <- c(sample_blogs, sample_news, sample_twitter)

#Create a clean corpus sample of all three files
corpus_vector <- VectorSource(dataset_sample)
clean_corpus <- Corpus(corpus_vector)

#Clean punctuation marks
clean_corpus <- tm_map(clean_corpus, removePunctuation)

#Remove numbers
clean_corpus <- tm_map(clean_corpus, removeNumbers)

#Remove extra whitespace
clean_corpus <- tm_map (clean_corpus, stripWhitespace)

#convert all letters to lowercase
clean_corpus <- tm_map(clean_corpus, tolower)
```



## References

This report uses the SwiftKey dataset which can be downloaded here: http://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip 